import json
import re
from typing import Any, Dict, Optional, List

import openai

from src.config import settings
from src.utils.logger import setup_logger

def _extract_first_json_object(text: str) -> Optional[Dict[str, Any]]:
    """
    [æ–°å¢å·¥å…·] ä» LLM è¾“å‡ºä¸­æå–ç¬¬ä¸€ä¸ª JSON å¯¹è±¡
    ç”¨äºæ”¯æŒ OpenAlexRetriever ä¸­çš„ Concept æ˜ å°„åŠŸèƒ½
    """
    if not text:
        return None

    # 1. ä¼˜å…ˆå¤„ç† Markdown ä»£ç å— ```json ... ```
    fenced = re.search(r"```(?:json)?\s*([\s\S]*?)\s*```", text, re.IGNORECASE)
    if fenced:
        candidate = fenced.group(1).strip()
        try:
            obj = json.loads(candidate)
            return obj if isinstance(obj, dict) else None
        except Exception:
            pass

    # 2. æ‰«æç¬¬ä¸€ä¸ªåŒ¹é… {} çš„ç‰‡æ®µ
    start = text.find("{")
    if start < 0:
        return None
    for end in range(len(text), start, -1):
        if text[end - 1] != "}":
            continue
        candidate = text[start:end]
        try:
            obj = json.loads(candidate)
            return obj if isinstance(obj, dict) else None
        except Exception:
            continue
    return None


class LLMService:
    """
    [Merged LLMService]
    ä¿ç•™äº†ä½ çš„æ¥å£ chat()ï¼Œå¢åŠ äº† chat_json() å’Œ OpenRouter æ”¯æŒã€‚
    """

    def __init__(self):
        self.logger = setup_logger("LLMService")
        self.model = settings.LLM_MODEL_NAME
        self.embedding_model = getattr(settings, "EMBEDDING_MODEL_NAME", "text-embedding-3-small")

        # åˆå§‹åŒ– Client (ä¿æŒä½ åŸæœ‰çš„é€»è¾‘)
        openrouter_key = getattr(settings, "OPENROUTER_API_KEY", None)
        openrouter_base = getattr(settings, "OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
        
        if openrouter_key:
            self.logger.info(f"Initializing LLM via OpenRouter")
            self.client = openai.OpenAI(
                api_key=openrouter_key,
                base_url=openrouter_base,
                default_headers={"X-Title": "ScholarRAG"}
            )
        else:
            self.logger.info(f"Initializing LLM via OpenAI-compatible")
            self.client = openai.OpenAI(
                api_key=settings.OPENAI_API_KEY,
                base_url=settings.OPENAI_BASE_URL
            )

    def chat(self, system_prompt: str, user_prompt: str) -> str:
        """
        æ ‡å‡†å¯¹è¯æ¥å£ (ä¿æŒä¸ä½ åŸæœ‰æ¥å£å®Œå…¨ä¸€è‡´)
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=settings.LLM_TEMPERATURE,
                max_tokens=settings.LLM_MAX_TOKENS,
            )
            
            # [å¢å¼º] é”™è¯¯æ£€æŸ¥
            choices = getattr(response, "choices", None) or []
            if not choices:
                msg = f"LLM Call Failed: empty choices (model={self.model})"
                self.logger.error(msg)
                return msg

            content = choices[0].message.content
            if content:
                return content
            
            return "LLM Call Failed: empty content"

        except Exception as e:
            error_msg = f"LLM Call Failed: {str(e)}"
            self.logger.error(error_msg)
            return error_msg

    def chat_json(self, system_prompt: str, user_prompt: str) -> Optional[Dict[str, Any]]:
        """
        [æ–°å¢] ç»“æ„åŒ–è¾“å‡ºæ¥å£ (OpenAlexRetriever éœ€è¦ç”¨åˆ°)
        """
        try:
            text = self.chat(system_prompt=system_prompt, user_prompt=user_prompt)
            return _extract_first_json_object(text)
        except Exception as e:
            self.logger.error(f"LLM JSON Call Failed: {str(e)}")
            return None
        
    def chat_stream(self, system_prompt: str, user_prompt: str):
        """
        [æ–°å¢] æµå¼å¯¹è¯æ¥å£
        è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ (Generator)ï¼Œé€ä¸ª token äº§å‡º
        """
        try:
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=settings.LLM_TEMPERATURE,
                max_tokens=settings.LLM_MAX_TOKENS,
                stream=True,  # <--- ä¿ç•™è¿™ä¸ª
                # stream_options={"include_usage": False}  <--- ğŸ”´ [åˆ é™¤è¿™ä¸€è¡Œ] è¿™ä¸€è¡Œå¯¼è‡´äº† 400 é”™è¯¯
            )
            
            for chunk in stream:
                if hasattr(chunk, 'choices') and chunk.choices:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'content') and delta.content:
                        yield delta.content

        except Exception as e:
            error_msg = f"\n[LLM Stream Failed: {str(e)}]"
            self.logger.error(error_msg)
            yield error_msg

    def get_embedding(self, text: str) -> List[float]:
        """
        [æ–°å¢] è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º (Embedding)
        """
        try:
            # ç§»é™¤æ¢è¡Œç¬¦ï¼Œé¿å…å½±å“ embedding è´¨é‡
            text = text.replace("\n", " ")
            
            response = self.client.embeddings.create(
                input=[text],
                model=self.embedding_model
            )
            return response.data[0].embedding
        except Exception as e:
            self.logger.error(f"Embedding Failed: {e}")
            # å¤±è´¥æ—¶è¿”å›é›¶å‘é‡çš„æ›¿ä»£æ–¹æ¡ˆæˆ–ç©ºåˆ—è¡¨ï¼Œè§†ä¸‹æ¸¸å¤„ç†è€Œå®š
            # è¿™é‡Œè¿”å›ç©ºåˆ—è¡¨è®©è°ƒç”¨è€…å¤„ç†
            return []