import pyalex
from pyalex import Works
from src.retrieval.base import BaseRetriever  # æ³¨æ„ï¼šåŸæ¥çš„ base.py å¼•ç”¨è·¯å¾„æ˜¯å¦æ­£ç¡®
from src.utils.logger import setup_logger
from src.config import settings  # <--- å¼•å…¥æ–°é…ç½®æ¨¡å—
import os

# ç›´æ¥ä» settings å¯¹è±¡é…ç½® pyalex
pyalex.config.email = settings.OPENALEX_EMAIL

class OpenAlexRetriever(BaseRetriever):
    def __init__(self):
        self.logger = setup_logger("OpenAlex")

    def _invert_abstract(self, inverted_index):
        """å°† OpenAlex çš„å€’æ’ç´¢å¼•é‡ç»„ä¸ºå¯è¯»æ–‡æœ¬"""
        if not inverted_index:
            return None
        max_len = 0
        word_map = {}
        for word, positions in inverted_index.items():
            for pos in positions:
                word_map[pos] = word
                if pos > max_len:
                    max_len = pos
        tokens = [word_map.get(i, "") for i in range(max_len + 1)]
        return " ".join(tokens)

    # def search(self, query: str, top_k: int = None, concept_ids: list = None) -> list:
    #     # å¦‚æœè°ƒç”¨æ—¶æ²¡æŒ‡å®š top_kï¼Œå°±ä½¿ç”¨å…¨å±€é…ç½®
    #     if top_k is None:
    #         top_k = settings.RAG_TOP_K
            
    #     self.logger.info(f"Searching OpenAlex for: '{query}' (Limit: {top_k})")
        
    #     search_query = Works().search(query).filter(has_abstract=True)
        
    #     if concept_ids:
    #         search_query = search_query.filter(concepts={"id": "|".join(concept_ids)})
    #         self.logger.info(f"Filtering by Concepts: {concept_ids}")

    #     search_query = search_query.filter(from_publication_date="2019-01-01")
        
    #     # è·å–ç»“æœ
    #     try:
    #         results = search_query.sort(cited_by_count="desc").get(per_page=top_k)
    #     except Exception as e:
    #         self.logger.error(f"API Request Failed: {e}")
    #         return []

    #     papers = []
    #     for work in results:
    #         abstract = self._invert_abstract(work.get("abstract_inverted_index"))
    #         if not abstract or len(abstract) < 50: 
    #             continue

    #     # --- [æ–°å¢] è·å– PDF é“¾æ¥ ---
    #     pdf_url = None
    #     best_oa = work.get("best_oa_location")
    #     if best_oa:
    #         pdf_url = best_oa.get("pdf_url")
        
    #     # æœ‰äº›æ—¶å€™ pdf_url åœ¨ primary_location é‡Œ
    #     if not pdf_url and work.get("primary_location"):
    #         pdf_url = work.get("primary_location").get("pdf_url")

    #     papers.append({
    #         "id": work["id"],
    #         "title": work["display_name"],
    #         "year": work["publication_year"],
    #         "cited_by": work["cited_by_count"],
    #         "abstract": abstract,
    #         "url": work.get("doi") or work.get("id"),
    #         "pdf_url": pdf_url,
    #         "authors": [a["author"]["display_name"] for a in work.get("authors", [])],
    #         "concepts": [c["display_name"] for c in work.get("concepts", [])[:3]]
    #     })
            
    #     self.logger.info(f"Found {len(papers)} valid papers.")

    #     # --- [æ–°å¢] è°ƒè¯•è¾“å‡ºåˆ° data/papers_debug.txt ---
    #     debug_path = os.path.join("data", "papers_debug.txt")
    #     # ç¡®ä¿ data ç›®å½•å­˜åœ¨
    #     os.makedirs("data", exist_ok=True) 
    #     with open(debug_path, "w", encoding="utf-8") as f:
    #         for p in papers:
    #             f.write(f"ID: {p['id']}\nTitle: {p['title']}\nPDF: {p['pdf_url']}\n{'-'*30}\n")
    #     self.logger.info(f"Saved metadata to {debug_path}")

    #     return papers

    def search(self, query: str, top_k: int = None, concept_ids: list = None) -> list:
        if top_k is None:
            top_k = settings.RAG_DOWNLOAD_K
            
        self.logger.info(f"Searching OpenAlex for: '{query}' (Limit: {top_k})")
        
        # 1. åŸºç¡€æœç´¢
        # Works().search(query) é»˜è®¤å°±æ˜¯æŒ‰ relevance_score æ’åºçš„ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼
        search_query = Works().search(query).filter(has_abstract=True)
        
        # 2. æ¦‚å¿µè¿‡æ»¤ (å¦‚æœæœ‰)
        if concept_ids:
            search_query = search_query.filter(concepts={"id": "|".join(concept_ids)})
            self.logger.info(f"Filtering by Concepts: {concept_ids}")

        # 3. å¹´ä»½è¿‡æ»¤ (ä¿ç•™æœ€è¿‘å‡ å¹´çš„)
        search_query = search_query.filter(from_publication_date="2020-01-01")
        
        try:
            # [å…³é”®ä¿®æ”¹] ç§»é™¤ .sort(cited_by_count="desc")
            # åªæœ‰å½“æ²¡æœ‰ query (çº¯æµè§ˆæ¨¡å¼) æ—¶ï¼Œæ‰éœ€è¦æŒ‰å¼•ç”¨æ’åºã€‚
            # è¿™é‡Œæˆ‘ä»¬æœ‰ queryï¼Œæ‰€ä»¥ç›¸ä¿¡ OpenAlex çš„ BM25 ç›¸å…³æ€§æ’åºã€‚
            results = search_query.get(per_page=top_k)
        except Exception as e:
            self.logger.error(f"API Request Failed: {e}")
            return []

        papers = []
        for work in results:
            abstract = self._invert_abstract(work.get("abstract_inverted_index"))
            if not abstract or len(abstract) < 50: 
                continue

            # --- PDF é“¾æ¥è·å–ç­–ç•¥ (ä¿æŒä½ ä¹‹å‰çš„å¢å¼ºç‰ˆé€»è¾‘) ---
            pdf_url = None
            
            # ç­–ç•¥ 1: Best OA
            best_oa = work.get("best_oa_location")
            if best_oa:
                pdf_url = best_oa.get("pdf_url")
            
            # ç­–ç•¥ 2: Primary Location
            if not pdf_url and work.get("primary_location"):
                pdf_url = work.get("primary_location").get("pdf_url")

            # ç­–ç•¥ 3: ArXiv Fallback
            if not pdf_url:
                ids = work.get("ids", {})
                arxiv_url = ids.get("arxiv")
                if arxiv_url:
                    pdf_url = arxiv_url.replace("/abs/", "/pdf/") + ".pdf"
                    self.logger.info(f"ğŸ”— Recovered ArXiv PDF for {work['id']}: {pdf_url}")

            papers.append({
                "id": work["id"],
                "title": work["display_name"],
                "year": work["publication_year"],
                "cited_by": work["cited_by_count"],
                "abstract": abstract,
                "url": work.get("doi") or work.get("ids", {}).get("openalex") or work.get("id"),
                "pdf_url": pdf_url,
                "authors": [a["author"]["display_name"] for a in work.get("authors", [])],
                "concepts": [c["display_name"] for c in work.get("concepts", [])[:3]]
            })
            
        self.logger.info(f"Found {len(papers)} valid papers.")

        # Debug è¾“å‡º
        debug_path = os.path.join("data", "papers_debug.txt")
        os.makedirs("data", exist_ok=True) 
        with open(debug_path, "w", encoding="utf-8") as f:
            for p in papers:
                status = "âœ… Has PDF" if p['pdf_url'] else "âŒ No PDF"
                f.write(f"[{status}] {p['id']} | {p['title']}\nURL: {p['url']}\nPDF: {p['pdf_url']}\n{'-'*30}\n")
        self.logger.info(f"Saved metadata to {debug_path}")

        return papers