# ui/logic.py
import sys
import os
import streamlit as st
import json # ç¡®ä¿å¯¼å…¥ json

# ç¡®ä¿èƒ½å¯¼å…¥ src
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.retrieval.openalex import OpenAlexRetriever
from src.retrieval.vector_store import LocalVectorStore
from src.graph.expansion import ConceptExpander
# ç¡®ä¿å¯¼å…¥åˆå¹¶åçš„ Generator (æ”¯æŒ papers_metadata å‚æ•°)
from src.core.generator import ReviewGenerator
from src.config import settings

# ä½¿ç”¨ Streamlit ç¼“å­˜æœºåˆ¶ï¼Œé¿å…æ¯æ¬¡åˆ·æ–°éƒ½é‡æ–°åˆå§‹åŒ–æ¨¡å‹
@st.cache_resource(show_spinner=False)
def get_engine():
    retriever = OpenAlexRetriever()
    local_store = LocalVectorStore()
    expander = ConceptExpander()
    generator = ReviewGenerator()
    return retriever, local_store, expander, generator

def recursive_summarize(generator, current_summary, new_messages):
    """
    é€’å½’æ‘˜è¦æ›´æ–°é€»è¾‘ (ä¿æŒä¸å˜)
    """
    if not new_messages:
        return current_summary

    new_dialogue = "\n".join([f"{m['role']}: {m['content']}" for m in new_messages])
    
    if current_summary:
        prompt = f"""
        You are a memory manager for a research assistant.
        Current Knowledge Summary: "{current_summary}"
        New Interaction to Integrate: {new_dialogue}
        Task: Update the summary to include key insights from the new interaction. 
        Keep it concise. Do not lose important previous context. No more than 200 words.
        Updated Summary:
        """
    else:
        prompt = f"""
        Summarize the key research questions and findings from this conversation concisely:
        {new_dialogue}
        """

    try:
        new_summary = generator.llm.chat("You are a helpful summarizer.", prompt)
        return new_summary
    except Exception as e:
        print(f"Summary failed: {e}")
        return current_summary

def generate_viral_copy(conversation_text):
    """
    ç”Ÿæˆç¤¾äº¤åª’ä½“æ–‡æ¡ˆ (ä¿æŒä¸å˜)
    """
    _, _, _, generator = get_engine()
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸ä»…æ‡‚å­¦æœ¯ï¼Œè¿˜æ·±è°™ç¤¾äº¤åª’ä½“ä¼ æ’­è§„å¾‹çš„â€œå­¦æœ¯åšä¸»â€ã€‚
    è¯·å°†ä»¥ä¸‹å¯¹è¯å†…å®¹æ€»ç»“ä¸ºä¸€æ®µé€‚åˆå‘å¸ƒåœ¨â€œçµæ„Ÿå¹¿åœºâ€çš„çŸ­æ–‡æ¡ˆã€‚
    
    [å¯¹è¯å†…å®¹]
    {conversation_text}
    
    [è¦æ±‚]
    1. **æç®€ä¸»ä¹‰**ï¼šæ€»å­—æ•°ä¸¥æ ¼æ§åˆ¶åœ¨ 150 å­—ä»¥å†…ã€‚
    2. **é‡‘å¥åŒ–**ï¼šç¬¬ä¸€å¥å¿…é¡»æ˜¯æŠ“äººçœ¼çƒçš„ Insight æˆ–åç›´è§‰ç»“è®ºã€‚
    3. **ç»“æ„åŒ–**ï¼šé‡‡ç”¨ "ğŸ’¡æ ¸å¿ƒè§‚ç‚¹ + ğŸ“Œä¸‰ä¸ªå…³é”®ç‚¹" çš„æ ¼å¼ã€‚
    4. **é£æ ¼**ï¼šä½¿ç”¨ Emoji å¢åŠ å¯è¯»æ€§ï¼Œè¯­æ°”è½»æ¾ä½†æœ‰æ·±åº¦ã€‚
    5. **è¾“å‡º**ï¼šç›´æ¥è¾“å‡ºæ–‡æ¡ˆå†…å®¹ï¼Œä¸è¦åŒ…å«"å¥½çš„"ã€"æ–‡æ¡ˆå¦‚ä¸‹"ç­‰åºŸè¯ã€‚
    """
    
    try:
        viral_copy = generator.llm.chat("You are a creative social media editor.", prompt)
        return viral_copy
    except Exception as e:
        return "ğŸ’¡ çµæ„Ÿæ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ç¼–è¾‘ã€‚"

def perform_retrieval(query, use_graph, history_context_str):
    """
    [é˜¶æ®µä¸€] æ‰§è¡Œæ£€ç´¢ä¸çŸ¥è¯†åº“æ„å»º
    è¿”å›: (context_chunks, source_papers, log_messages)
    """
    retriever, local_store, expander, generator = get_engine()
    logs = [] 
    
    # 1. å¹¿åº¦æœç´¢ (Graph Expansion)
    concept_ids = None
    if use_graph:
        logs.append("æ­£åœ¨æ„å»ºæ¦‚å¿µå›¾è°± (Graph Expansion)...")
        info = expander.expand_query(query)
        if info:
            concept_ids = [info['id']]
            logs.append(f"è¯†åˆ«æ ¸å¿ƒæ¦‚å¿µ: {info['name']}")
    
    # 2. OpenAlex æ£€ç´¢
    logs.append("æ­£åœ¨ OpenAlex æ£€ç´¢æƒå¨æ–‡çŒ®...")
    # æ³¨æ„ï¼šè¿™é‡Œ retrieve çš„ papers å·²ç»åŒ…å«äº† teammate çš„ PageRank é€»è¾‘
    papers = retriever.search(query, top_k=settings.RAG_DOWNLOAD_K, concept_ids=concept_ids)
    
    if papers:
        logs.append(f"æ£€ç´¢åˆ° {len(papers)} ç¯‡ç›¸å…³æ–‡çŒ®ï¼Œå‡†å¤‡é˜…è¯»...")
        # 3. å…¥åº“ (è€—æ—¶æ“ä½œ)
        local_store.add_papers(papers)
    else:
        logs.append("æœªæ£€ç´¢åˆ°æ–°æ–‡çŒ®ï¼Œå°†åŸºäºç°æœ‰çŸ¥è¯†åº“å›ç­”ã€‚")

    # 4. æ·±åº¦å¬å›
    logs.append("æ­£åœ¨è¿›è¡Œå‘é‡é‡æ’ä¸ä¸Šä¸‹æ–‡æ„å»º...")
    chunks = local_store.search(query, top_k=settings.RAG_RETRIEVAL_K)
    
    return chunks, papers, logs

# [å…³é”®ä¿®æ”¹] å¢åŠ äº† papers_metadata å‚æ•°
def get_response_stream(query, mode, history_context_str, chunks, language="Chinese", papers_metadata=None):
    """
    [é˜¶æ®µäºŒ] ç”Ÿæˆæµå¼å›ç­”
    è¿”å›: generator (yield string)
    """
    _, _, _, generator = get_engine()
    
    # 5. æ‹¼æ¥ Context
    augmented_query = f"""
    [Conversation History Context]:
    {history_context_str}
    
    [Current User Question]: 
    {query}
    """
    
    # 6. è°ƒç”¨ Generator çš„æµå¼æ¥å£
    # [å…³é”®] åªæœ‰åœ¨ review æ¨¡å¼ä¸‹æ‰ä¼ å…¥ papers_metadataï¼Œç”¨äºç»“æ„åŒ–ç»¼è¿°
    meta_to_pass = papers_metadata if mode == "review" else None
    
    return generator.generate_stream(
        augmented_query, 
        chunks, 
        task_type=mode, 
        language=language,
        papers_metadata=meta_to_pass # é€ä¼ ç»™ generator
    )

def generate_follow_up_questions(history_context_str):
    """
    ç”Ÿæˆåç»­è¿½é—® (ä¿æŒä¸å˜)
    """
    _, _, _, generator = get_engine()
    
    prompt = f"""
    You are a helpful research assistant. Based on the conversation history below, suggest 3 short, relevant academic follow-up questions that the user might want to ask next.
    
    [Conversation History]
    {history_context_str}
    
    [Requirements]
    1. Output strictly a JSON list of strings. Example: ["Question 1?", "Question 2?", "Question 3?"]
    2. Questions should be concise (under 20 words).
    3. Language: Match the language of the conversation (Chinese/English).
    4. Focus on digging deeper, clarifying concepts, or exploring related fields.
    """
    
    try:
        response = generator.llm.chat("You are a follow-up question generator.", prompt)
        clean_text = response.replace("```json", "").replace("```", "").strip()
        questions = json.loads(clean_text)
        
        if isinstance(questions, list):
            return questions[:3]
        return []
    except Exception as e:
        print(f"Follow-up generation failed: {e}")
        return []